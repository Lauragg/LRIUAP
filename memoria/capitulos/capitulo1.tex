% !TeX root = ../libro.tex
% !TeX encoding = utf8

\chapter{Red neuronal}
\section{El problema de clasificar una imagen.}

Cuando observamos una imagen, podemos localizar varios elementos, a partir de los cuáles, esta se encuentra compuesta con tan sólo un vistazo. Sin embargo, para un ordenador no se trata de algo tan sencillo puesto que sólo es capaz de ver un gran conjunto de números que no tienen por qué tener relación alguna entre sí.\newline

\begin{figure}[htpb]
  \centering
  \includegraphics[width=0.8\textwidth]{blanquita}
  \caption{Cómo un ordenador ve una imagen}
  \label{fig:blanquita}
\end{figure}

Idealmente, en esta imagen desearíamos que fuera capaz de identificar que se trata de un perro, en concreto de un chiguagua, de pelaje blanco y manchas color café que se encuentra sobre un césped. Estos pocos datos, que para nosotros parecen tan triviales, necesitan de horas y horas de computación para ser obtenidos a partir de una imagen cualquiera.\newline

Dejamos todos estos detalles, a los cuales esperamos poder llegar en secciones futuras, y simplificamos el problema a tener un conjunto de etiquetas y buscar con cuál de todas ellas tiene mayor relación nuestra imagen.\newline

Una primera idea podría ser utilizar utilizar \emph{$k$-nearest neighbor classifier}, en adelante $k$-NN. Este clasificador consiste en, para cada etiqueta, determinar las $k$ imágenes cuya distancia, siendo Manhattan y Euclídea las más comunes, entre los valores de los píxeles de nuestra imagen sea menor. La etiqueta idónea será aquella cuya dicha distancia sea menor considerando las $k$ imágenes.\\

%Este clasificador consiste en que, para cada etiqueta, se correspondan $k$ imágenes y consideraremos como etiqueta idónea para nuestra clasificación aquella cuya distancia, siendo Manhattan y Euclídea las más comunes, entre los píxeles con nuestra imagen sea menor. Nos encontramos con que el tiempo de entrenamiento de nuestro clasificador sería ínfimo en comparación con el tiempo de clasificación, si bien se pueden hacer diversas mejoras que cambien estos hechos. \newline

\begin{figure}[htpb]
  \centering
  \includegraphics[width=0.8\textwidth]{knn}
  \caption{Un ejemplo de la diferencia entre un \emph{Nearest Neighbor classifier} y un \emph{5-Nearest Neighbor classifier} con puntos bidimensionales y tres clases.\cite{stanford}}
  \label{fig:knn}
\end{figure}

De esta sencilla propuesta surgen diversos problemas. Se suele dar mayor importancia al valor concreto de los píxeles, en lugar de a las formas que de las que está compuesta la figura, provocando que valores como los colores de fondo puedan influir más en la clasificación que los propios píxeles de la figura que queremos clasificar. En el ejemplo de la imagen del chiguagua, si considerásemos las etiquetas verde y perro, tendríamos que por lo general como respuesta el color verde, pese a que estamos más interesados por la mascota en sí. Otro gran problema sería la baja escalabilidad que nos proporciona esta solución, al incrementarse enormemente el costo computacional de clasificación conforme aumenta el número de imágenes a comparar, ya sea por aumentar el número de etiquetas o nutrir de más datos las ya existentes.\newline

%El siguiente paso, es buscar una forma de ``memorizar'' los datos de entrenamiento de forma que no tengamos que estar comparándolos con todos ellos cuando queramos clasificar una imagen. Lo que buscamos es poder valorar de alguna forma la imagen completa y a partir de esta ``puntuación'' conocer qué etiqueta le corresponde mejor a nuestra imagen. De esta forma, veríamos drásticamente reducido el tiempo de clasificación, sacrificando para ello el tiempo de entrenamiento, y podríamos ampliar en varias unidades la cantidad de datos de entrenamiento utilizados, aumentando así la precisión de nuestro clasificador.

Debido a la falta de escalabilidad de este método de clasificación no paramétrico, existe un gran problema ante el incremento de los datos de entrenamiento. Un primer ejemplo de la búsqueda de aumentar la escalabilidad del algoritmo sin un aumento de los tiempos de clasificación, podrían ser los clasificadores lineales.\\

\begin{figure}[htpb]
  \centering
  \includegraphics[width=0.8\textwidth]{pixelspace}
  \caption{Ejemplo de representación de un clasificador lineal con tres etiquetas.\cite{stanford}}
  \label{fig:pixelspace}
\end{figure}

Consideremos $D \in \N$ la dimensión de nuestras imágenes o datos de entrada y $K \in \N$ la cantidad de etiquetas o categorías bajo las cuales pueden ser clasificadas. Siendo $N \in \N$ el número de ejemplos que utilizaremos para entrenar nuestro clasificador, tendremos que para cada dato de entrenamiento $x_i \in \R^D \; i=1,...,N$ le corresponde una etiqueta $y_i \in \{1,...,K\}$ tal que juntos conforman el par $(x_i,y_i)$ de imagen y categoría a la que pertenece. Utilizando estos datos, podremos entrenar el clasificador lineal que será una función del tipo $$f(x)=W\cdot x+ b\; \; \; W\in M_{K,D}(\R) \;\; b \in \R^K \;\; \forall x\in \R^D,$$ con la cual estaremos dividiendo el espacio de resultados utilizando hiperplanos. Existen múltiples tipos de clasificadores lineales, como por ejemplo una \emph{máquina de vectores de soporte multiclase (Multiclass SVM)} o un clasificador SoftMax que tienen como principal diferencia la función de pérdida que utilizan para penalizar las etiquetas incorrectas.


%\section{Clasificadores Lineales}

%Antes de nada, vamos a comenzar contextualizando matemáticamente el entorno en el que nos encontramos. Una vez realizado esto podremos hablar correctamente de los clasificadores lineales y así poder extenderlos naturalmente a los conceptos de Red Neuronal y de Red Neuronal Convolucionada.\newline

%Sea $D \in \N$ la dimensión de nuestras imágenes o datos de entrada, por lo general el número de píxeles que estas poseen, y $K \in \N$ la cantidad de etiquetas o categorías bajo las cuales pueden ser clasificadas. Siendo $N \in \N$ el número de ejemplos que utilizaremos para entrenar nuestro clasificador, tendremos que para cada dato de entrenamiento $x_i \in \R^D \; i=1,...,N$ le corresponde una etiqueta $y_i \in 1,...,K$ tal que juntos conforman el par $(x_i,y_i)$ de imagen y categoría a la que pertenece.\newline

%Para que no sea más fácil de entender este contexto, tomaremos como ejemplo el conjunto de datos CIFAR-10 que consiste en 60000 imágenes RGB de dimensión 32x32 y 10 categorías. De esta forma, tendríamos que $D=32\cdot 32 \cdot 3 = 3072$, $K=10$ y $N=60000$.

%\begin{definicion}\label{def:ScoreFunction}
%Definiremos \emph{función de puntuación} o \emph{score function} como una función $f:\R^D \to \R^K$ que asigna los píxeles de la imagen sin procesar una serie de puntuaciones para cada etiqueta.
%\end{definicion}

%Una función de puntuación nos revela la probabilidad que tiene cada imagen de pertenecer a cada una de la distintas etiquetas de las que disponemos. Podemos definir un \emph{clasificador lineal} o \emph{linear classifier} \label{def:LinearClassifier} como aquel que utiliza una función de puntuación lineal, es decir, de la forma:

%$$f(x)=W\cdot x+ b\; \; \; W\in M_{K,D}(\R) \;\; b \in \R^K \;\; \forall x\in \R^D$$

%Dicho esto, existen diferentes tipos de clasificadores lineales y la principal diferencia entre ellos reside en la función de pérdida que utilicen a la hora de entrenar la red.

%\begin{definicion}\label{def:LossFunction}
%Una \emph{función de pérdida} o \emph{loss function} es aquella que durante el entrenamiento de un clasificador se encarga de penalizar las etiquetas incorrectas.
%\end{definicion}

%FIXME: ¿Hablar de SVM, SoftMax y Cross-Entropy?\newline

%Llegados a este punto, debemos de explicar cómo funcionan los clasificadores lineales, es decir, cómo estos son entrenados y para qué se emplean las funciones de puntuación y de pérdida. Dado poseemos un conjunto de ejemplos con su correspondiente etiqueta pero desconocemos el valor de los parámetros $W$ y $b$, nuestro objetivo es utilizar dichos datos de entrenamiento para estimar estos valores. \newline

%Para ello, comenzamos haciendo una conjetura sobre un posible valor para nuestros parámetros, evaluamos nuestra función de puntuación con todos nuestros datos de entrenamiento y seguidamente utilizamos la función de pérdida para estimar cómo de bien funciona nuestra conjetura. Analizamos los resultados y hacemos una nueva conjetura que los mejore, repitiendo el proceso un número lo suficientemente grande de veces.\newline

%Seguidamente, debemos preguntarnos cómo realizamos la nueva conjetura de forma que nos aseguremos tener unos resultados mejores. Hacerlo de forma totalmente aleatoria, repitiéndolo hasta obtener una pérdida lo suficientemente pequeña, no parece una buena idea puesto que es difícil saber cuándo encontraremos una buena respuesta. Entre las distintas técnicas, nos encontramos con las basadas en la \emph{búsqueda aleatoria local} y las que se basan en el \emph{Gradiente Descendiente}, entre otras muchas. Como ejemplo, tomaremos el algoritmo del gradiente descendiente y minimizaremos la función de pérdida para llegar a aquellos pesos que menor error nos den.\newline

%FIXME: Enlazar pdfs que expliquen bien cómo funciona el gradiente descendiente y la búsqueda aleatoria local \newline
%\lstset{language=Python}
%\begin{lstlisting}[frame=single]
%while condicion_de_parada :
%  weight_grad=evaluate_grad(loss_fun,x,y)
%  weight = -step_size*weight_grad

%\end{lstlisting}

%El código mostrado más arriba se trata de una versión muy simplificada de cómo funcionaría de cómo podríamos implementar el algoritmo, teniendo en cuenta que nosotros mismos podremos modificar la condición de parada de acuerdo a nuestras necesidades y que el valor \emph{step\_size} es algo que podremos fijar de la misma forma, sabiendo que este nos indica cuánto queremos avanzar en la dirección que nos indica el gradiente. FIXME: Enlazar pdfs que expliquen la importancia de estos valores y cómo fijarlos.\newline

%FIXME: Añadir implementación en Tensorflow de un clasificador lineal.

%\begin{ejemplo}\label{SoftMaxBinary}
%Fijemos una etiqueta, $y=0$ por lo que $K=1$. Tendríamos que nuestra función de puntuación sería $f:\R^D \to \R$ donde $W=(w_1,...w_D)\in \R^D$ y $b\in \R$ luego $$f(x)=W\cdot x + b = \sum_{j=1}^D w_j x_j+b.$$ Así mismo, tomamos como función de pérdida $$L(x,f)=-\ln \frac{1}{\sum_{j=1}^D e^{f_j}}=-\ln \frac{1}{\sum_{j=1}^D e^{w_j x_j +b}}=\ln\sum_{j=1}^D e^{w_j x_j +b}$$ que es un caso particular de \emph{Entropía Cruzada} o \emph{Cross-entropy}.\newline

%Este clasificador recibe el nombre de \emph{Binary Softmax classifier} o \emph{Binary Logistic Regression classifier}. De esta forma, si consideramos la función probabilística \emph{sigmoide} tendríamos que el minimizar la función de pérdida estaríamos aumentando la probabilidad clasificar correctamente nuestros datos puesto que: $$P(y=0 | x; W)=\sigma (f(x)) = \frac{1}{1+e^{-f(x)}}.$$

%\begin{observacion}
%Estamos calculando la probabilidad de coincidir o no con una determinada etiqueta, recibe el nombre de binario porque también podríamos considerar que tenemos dos etiquetas y que si no perteneces a una forzosamente perteneces a la otra. Este modelo puede por tanto construirse también utilizando ambas etiquetas y con mejores resultados a la hora de clasificar puesto que durante el entrenamiento la función de pérdida es ligeramente modificada para tener en cuenta la otra etiqueta. En cualquier %caso, ambas construcciones siguen la misma interpretación probabilística y, además, tenemos que $P(y=1 | x; W)=1-P(y=0 | x; W)$.
%\end{observacion}
%\end{ejemplo}

%\begin{ejemplo}\label{SVM}
%Tomamos como función de pérdida $L(x,f)=\frac{1}{N} \sum_{i=1}^N \sum_{j \neq y_i} \max(0,f_j-f_{y_i} + \Delta)$ donde $\Delta \in \R$ es un parámetro que se suele ajustar utilizando una técnica llamada \emph{validación cruzada} o \emph{cross validation}. Esta función suele recibir el nombre de \emph{hinge loss} y el clasificador lineal es conocido como \emph{Multiclass Suport Vector Machine (SVM)}. Este clasificador "quiere" que la etiqueta correcta para cada imagen tenga una puntuación mayor %que las incorrectas por un margen fijo $\Delta$.
%\end{ejemplo}
\section{El modelo de una neurona}

Cuando comenzamos planteando nuestro problema buscábamos conseguir clasificar una imagen con una probabilidad de acertar similar o superior a la humana. Teníamos el problema de que un ordenador no era capaz de pensar o razonar de la misma forma que un ser humano y buscábamos una forma de clasificar esta información a pesar de ello. Es aquí donde consideraremos los modelos de redes neuronales, que buscan ser capaces de simular cómo funciona un cerebro humano para aprender cómo reconocer distintos elementos de la misma forma que nosotros lo hemos ido haciendo a lo largo de nuestra vida. %Es aquí donde nacen los modelos de redes neuronales, en un intento por ser capaces de simular cómo funciona nuestro propio cerebro en nuestros clasificadores y que estos sean capaces de aprender cómo reconocer los distintos elementos de la misma forma que nosotros lo hemos ido haciendo a lo largo de nuestra vida.\newline

\begin{figure}[htpb]
  \centering
  \includegraphics[width=0.45\textwidth]{neuron}
  \vrule
  \includegraphics[width=0.45\textwidth]{neuron_model}
  \caption{Comparación entre una neurona biológica (izquierda) y el modelo matemático (derecha) \cite{stanford} }
  \label{fig:neurona}
\end{figure}

Nuestro cerebro está formado por múltiples neuronas interconectadas entre sí que están constantemente transmitiéndose información y aprendiendo a través de todos los datos que reciben. Si nos fijamos en \ref{fig:neurona} podemos ver que una neurona real esta formada por dentritas que son quienes, a través del proceso de sinapsis, reciben la entrada de información, que es asimilada y transformada por su núcleo antes de ser transmitida a la siguientes neuronas a través de las divisiones de su axón en caso de que supere un cierto umbral y se active.\newline

Si consideramos que tenemos $D$ dentritas y que por la $i$-ésima dentrita recibimos la información $x_i$ con un peso o fuerza de sinapsis $w_i$, tendríamos que nuestro núcleo trabaja con el vector de información $(w_1 x_1,...,w_D x_D)$ que podemos estimar como $\sum_i w_i x_i $ y sumarle una determinada constante $b$ propia de la neurona. El umbral de activación y la señal enviada al resto de neuronas será la evaluación a través de una \emph{función de activación} \autoref{def:ActivationFunction}. Cabe destacar la similitud existente entre el modelo de una neurona y un clasificador lineal.\newline

\begin{definicion}\label{def:ActivationFunction}
Una \emph{función de activación} es una función $f: \R^n \to \R^n$ siendo $n\in\N$ el número de neuronas de la capa a evaluar.
\end{definicion}

A continuación, mencionaremos los ejemplos más representativos utilizados como funciones de activación:

%FIXME: Sugerencia: Añadir una gráfica para cada función de activación
\begin{itemize}
\item \emph{Función sigmoide} o \emph{logística} $\sigma: \R^n \to [0,1]^n$ definida como $\sigma_i(x_i)=\frac{1}{1+e^{-x_i}} $. Se suele utilizar en la última capa de nuestra red, cuando nuestras imágenes pueden pertenecer a varias clases o etiquetas al mismo tiempo al ser el resultado para cada componente independiente del resto.
\item \emph{Función softmax} utilizada normalmente en la última capa donde hay tantas neuronas como etiquetas y en el problema de clasificación donde una imagen puede pertenecer únicamente a una sola etiqueta o caso. Se trata de una modificación de función logística que normaliza la salida para obtener la probabilidad de pertenecer a cada clase obteniendo así que la suma de todas las salidas de esta capa sería $1$. Aquí, la $i$-ésima neurona tendría función de activación $\sigma_i(x)=\frac{e^{x_i}}{\sum_{j=1}^K e^{x_j}}$. % y utilizaría $L(x)=\frac{1}{N}\sum_{i=1}^N -\ln \frac{e^{\sigma_{y_i}(x)}}{\sum_{j=1}^D e^{\sigma_j}(x)}=\frac{1}{N}\sum_{i=1}^N (-{\sigma_{y_i}(x)}+\ln \sum_{j=1}^D e^{\sigma_j(x)})$ como función de pérdida para el entrenamiento.
\item \emph{Función ReLU}, cuyo nombre completo sería \emph{Rectified Linear Unit}. Se suele utilizar en las capas intermedias de nuestra red y es de la forma $f(x)=\max(0,x)$.
\item \emph{Función tanh} se trata de una centralización de la función sigmoide. $Tanh(x)=2\sigma(x)-1$.
\end{itemize}

\section{La red completa}

Una red red neuronal es un conjunto de neuronas dividas en varias \emph{capas} de forma que las neuronas de una capa pueden estar unidas o no con las neuronas de la capa anterior y de la siguiente formando así un grafo acíclico dirigido.\newline

\begin{figure}[htpb]
  \centering
  \includegraphics[width=0.4\textwidth]{neural_net}
  \vrule
  \includegraphics[width=0.5\textwidth]{neural_net2}
  \caption{Ejemplos de redes totalmente conectadas (\emph{fully-connected}) \cite{stanford} }
  \label{fig:fully-connected}
\end{figure}

La primera capa recibe el nombre de \emph{capa de entrada} que posee $D$ neuronas, es decir, tantas como la dimensión de nuestros datos, y puede haber varias de ellas. Por otro lado tenemos la \emph{capa de salida} con un número de neuronas dependiente de la cantidad de etiquetas que tengamos. El resto de neuronas se distribuyen dentro de las \emph{capas ocultas} cuya distribución y conexiones dependen del modelo en concreto que queramos desarrollar, quedando a nuestro criterio.\newline


Así, podemos afirmar que una red neuronal es la aplicación sucesiva de funciones de la forma

$$F(x)=\sum_{j=1}^n \alpha_j \sigma(w_j^Tx+b_j) \;\;\; \forall x \in \R^n,$$

donde $w_j\in\R^n,\;\; \alpha_j,b_j\in\R$ serán fijas una vez entrenada la red y $\sigma : \R \to \R$ la función de activación elegida en cada capa.\\

\subsection{Función de pérdida y métricas}
La función de pérdida, o \emph{loss function}, se trata de la función objetivo que querremos minimizar en el problema de optimización planteado durante el entrenamiento de una red neuronal. Existen muchas funciones de pérdida, que se pueden englobar en tres grandes categorías:
\begin{itemize}
\item Funciones de pérdida probabilísticas. Son aquellas cuyo resultado es interpretable como una medida de probabilidad. Un ejemplo de esto es la entropía cruzada, o \emph{cross entropy}, que mide la media de bits necesarios para identificar un evento.
\item Funciones de pérdida de regresión. Son aquellas cuyos valores son reales y continuos. Como ejemplos se tiene el error cuadrático medio o la similitud del coseno.
\item Funciones de pérdida para clasificaciones "máximo-margen". Un ejemplo de este tipo es la función de pérdida de bisagra, o \emph{hinge loss function}.
\end{enumerate}

Existen varias formas de medir el funcionamiento de una red neuronal sin que estas medidas lleguen a afectar al entrenamiento. Hay múltiples métricas de evaluación que se pueden utilizar en diferentes contextos, siendo la más común la precisión, exactitud o \emph{accuracy}.\\

La métrica de \emph{accuracy} corresponde al número de datos correctamente predichos con respecto a todos los puntos. Formalmente, en el caso binario, está definida como el número de verdaderos positivos y negativos divido entre los falsos positivos, verdaderos positivos, falsos negativos y verdaderos negativos.\\
\begin{figure}[htpb]
  \centering
  \includegraphics[width=0.6\textwidth]{accuracy}
  \caption{Exactitud o \emph{accuracy}}
  \label{fig:accuracy}
\end{figure}

Por otro lado, se mencionará también la intersección sobre la unión, mayormente conocida como \emph{intersection over union (IoU)}, que contabiliza el área de intersección y de unión entre las predicciones y los verdaderos valores para después dividir el primero entre el segundo.
\begin{figure}[htpb]
  \centering
  \includegraphics[width=0.6\textwidth]{iou}
  \caption{intersección sobre la unión o \emph{intersection over union (IoU)}}
  \label{fig:iou}
\end{figure}

\subsection{Propagación y optimización}
Con el fin de simplificar la explicación de los algoritmos que se verán en esta subsección, nos centraremos en el caso de las redes neuronales prealimentadas, o \emph{feed forward neural networks}, que son aquellas donde las conexiones entre las distintas neuronas no forman un ciclo. También son conocidas como redes neuronales hacia adelante puesto que la información únicamente se mueve hacia adelante, diferenciándose de las redes neuronales recurrentes.\\

El algoritmo que computa el avance de la información a través de estas redes recibe el nombre de propagación hacia adelante, o \emph{forward propagation}, \cite{Goodfellow-et-al-2016}. Durante el entrenamiento, la información de entrada continua hasta producir un coste escalar $J(\theta)$ y el algoritmo de propagación hacia atrás, o \emph{back propagation}, permite que ese coste fluya hacia atrás en la red para calcular el gradiente. \\

\begin{algorithm}
\caption{Propagación hacia adelante mediante una red neuronal típica, totalmente conectada, y el cálculo de la función de coste. La función de pérdida $L(\hat{y},y)$ depende de la salida de la red $\hat{y}$ y del objetivo $y$. Para obtener el coste total $J$, a la pérdida se le debería de añadir un regularizador $\Omega(\theta)$, donde $\theta$ contiene todos los parámetros (pesos y sesgos). \cite{Goodfellow-et-al-2016}}
\begin{algorithmic}\label{alg:forward}
\REQUIRE Profundidad de la red $l$
\REQUIRE $W^{(i)}, i\in\{1,...,l\}$, las matrices de pesos del modelo
\REQUIRE $b^{(i)}, i\in\{1,...,l\}$, los parámetros de sesgos del modelo
\REQUIRE $x$, la entrada al proceso
\REQUIRE $y$, la salida objetivo
\STATE $h^{(0)}=x$
\FOR{$k=1,...,l$}
\STATE $a^{(k)}=b^{(k)}+W^{(k)}h^{(k-1)}$
\STATE $h^{(k)}=\sigma(a^{(k)})$
\ENDFOR
\STATE $\hat{y}=h^{(l)}$
\STATE $J=L(\hat{y},y)+\lambda\Omega (\theta)$
\end{algorithmic}
\end{algorithm}

A menudo, se piensa que con la propagación hacia atrás se está realizado todo el proceso de aprendizaje de la red, cuando este algoritmo únicamente calcula el valor del gradiente y será otro algoritmo, como, por ejemplo, el descenso del gradiente estocástico (\emph{stochastic gradient descent}), es el utilizado para realizar el aprendizaje por la red. Los algoritmos utilizados para calcular este aprendizaje reciben el nombre de optimizadores.\\

El algoritmo de propagación hacia atrás realiza el cálculo de la regla de la cadena del cálculo, en un orden específico de operaciones que es muy eficiente, para calcular las derivadas de funciones compuestas por otras funciones cuyas derivadas son conocidas. \\
\begin{algorithm}
\caption{Computación hacia atrás para la red neuronal profunda del algoritmo \autoref{alg:forward}, que usa, en adición a la entrada $x$, un objetivo $y$. \cite{Goodfellow-et-al-2016}}
\begin{algorithmic}
\STATE Después del cálculo hacia adelante, calculamos el gradiente de la capa de salida:
\STATE $g\leftarrow\nabla_{\hat{y}}J=\nabla_{\hat{y}}L(\hat{y},y)$
\FOR{$k=l,l-1,...,1$}
\STATE Se convierte el gradiente de la salida de las capas en un gradiente en la función de activación:
\STATE $g\leftarrow \nabla_{a^(k)}J=g\odot\sigma'(a^{(k)})$
\STATE Se calcula el gradiente el los pesos y sesgos, incluidos los términos de regularización cuando estos sean necesarios:
\STATE $\nabla_{b^{(k)}}J=g+\lambda\nabla_b^(k)\Omega(\theta)$
\STATE $\nabla_{W^{(k)}}J=gh^{(k-1)}+\lambda\nabla_W^{(k)}\Omega(\theta)$
\STATE Se propaga el gradiente.
\STATE $g\leftarrow \nabla_{h^{(k-1)}}J=W^{(k)T}g$
\ENDFOR
\end{algorithmic}
\end{algorithm}

Todo esto es para poder plantear el objetivo más importante del entrenamiento de una red neuronal, el resolver un problema de optimización al querer minimizar el valor de la función de pérdida. \\

Para ello, se describirá el algoritmo del descenso del gradiente estocástico, o \emph{stochastic gradient descent (SGD)}, que, junto a sus variantes, son los algoritmos de optimización más utilizados en el aprendizaje profundo. \\

\begin{algorithm}
\caption{Descenso del gradiente estocástico con cantidad de movimiento, o \emph{momentum}. \cite{Goodfellow-et-al-2016}}
\begin{algorithmic}
\REQUIRE Ratio de aprendizaje $\epsilon$ y \emph{momentum} $\alpha$.
\REQUIRE Parámetro inicial $\theta$ y velocidad inicial $v$.
\WHILE{no se cumpla el criterio de parada}
\STATE Sea $\{x^{(1)},...,x^{(m)}\}$  un subconjunto del conjunto de entrenamiento con sus correspondientes etiquetas $y^{(i)}$.
\STATE Calcular la estimación del gradiente: $g\leftarrow \frac{1}{m}\nabla_\theta \sum_i L(f(x^{(i)};\theta),y^{(i)})$.
\STATE Calcular la actualización de la velocidad: $v \leftarrow \alpha v - \epsilon g$.
\STATE Aplicar actualización: $\theta \leftarrow \theta + v$.
\ENDWHILE
\end{algorithmic}
\end{algorithm}

Finalmente, describiremos la variante \emph{Adam} del \emph{SGD} puesto que será la que utilicemos como optimizador más adelante.\\

\begin{algorithm}
\caption{El algoritmo \emph{Adam}. \cite{Goodfellow-et-al-2016}}
\begin{algorithmic}
\REQUIRE Tamaño de paso $\epsilon$. (Sugerido por defecto: 0.0001)
\REQUIRE Ratio exponencial de caída para las estimaciones de momentos, $\rho_1$ y $\rho_2$ en $[0,1)]$. (Sugerido por defecto $0.9$ y $0.999$ respectivamente)
\REQUIRE Una pequeña constante $\delta$ utilizada para una estabilización numérica. (Sugerida: $10^{-8}$)
\REQUIRE Un parámetro inicial $\theta$0
\STATE Incializar las variables de momentos $s=0$ y $r=0$.
\STATE Inicializar el paso de tiempo $t=0$.
\WHILE{No se cumple el criterio de parada}
\STATE Sea $\{x^{(1)},...,x^{(m)}\}$  un subconjunto del conjunto de entrenamiento con sus correspondientes etiquetas $y^{(i)}$.
\STATE Calcular la estimación del gradiente: $g\leftarrow \frac{1}{m}\nabla_\theta \sum_i L(f(x^{(i)};\theta),y^{(i)})$.
\STATE Actualizar la estimación del primer momento sesgado: $s\leftarrow \rho_1 s+(1-\rho_1)g$.
\STATE Actualizar la estimación del segundo momento sesgado: $r\leftarrow \rho_2 r +(1-\rho_2) \odot g$.
\STATE Corregir el sesgo del primer momento: $\hat{s} \leftarrow \frac{s}{1-\rho_1^t}$.
\STATE Corregir el sesgo del segundo momento: $\hat{r} \leftarrow \frac{r}{1-\rho_2^t}$.
\STATE Calcular la actualización: $\Delta\theta = -\epsilon \frac{\hat{s}}{\sqrt{\hat{r}}+\delta}$ (Operaciones aplicadas elemento a elemento)
\STATE Aplicar la actualización: $\theta \leftarrow \theta + \Delta \theta$
\ENDWHILE
\end{algorithmic}
\end{algorithm}
Para más detalles sobre estos algoritmos, se recomienda la consulta de los libros \emph{Deep Learning} \cite{Goodfellow-et-al-2016} y \emph{Learning from data} \cite{10.5555/2207825}. \\
\chapter{Teoremas de Aproximación Universal}
Hasta ahora se ha definido una estructura con una serie de parámetros con la esperanza de poder llegar a resolver el problema planteado. Sin embargo, en ningún momento hemos utilizado ningún resultado que nos asegure que dicha estructura puede llegar a resolver el problema que deseamos. A continuación, mostraremos diversos resultados, con diversos niveles de generalidad, que muestran cómo esta estructura es un aproximador universal y por ello resuelve nuestro problema. Estos resultados se dividen en dos grandes categorías \emph{anchura indeterminada} y \emph{profundidad indeterminada}.

\section{Anchura indeterminada}
\subsection{George Cybenko, 1989}
Cybenko, en \cite{cybenko1989approximation}, demostró la capacidad de aproximación en el caso de anchura indeterminada y profundidad fijada para funciones de activación sigmoidales \autoref{def:sigmoidal}. A partir de esta versión clásica, se logró considerar otras funciones de activación, e incluso, demostrar que era gracias a la arquitectura en sí misma, y no a la función de activación, que las redes neuronales eran aproximadores universales \cite{Kurt1991251}. Aquí estudiaremos la versión del teorema para funciones continuas, pero debemos destacar que en el mismo artículo se mencionan también versiones para otros espacios de funciones.
\begin{definicion}\label{def:sigmoidal}
Una función $\sigma : \R \to \R$ es \emph{sigmoidal} si $$\lim_{t\to +\infty} \sigma(t)=1 \;\;\; y \;\;\;\; \lim_{t\to -\infty} \sigma(t)=0.$$
\end{definicion}\\

\begin{definicion}
Dada una $\sigma$-álgebra $M$, una \emph{medida con signo $\mu$} sobre $M$ es una función del conjunto $\mu: M\to [-\infty,+\infty] \sigma$-aditiva.
\end{definicion}
\begin{definicion}
Una medida se dice \emph{de Borel} si está definida sobre una $\sigma$-álgebra de Borel, es decir, la engendrada por los abiertos del espacio.
\end{definicion}
\begin{definicion}
Una \emph{medida con signo regular de Borel} sobre una $\sigma$-álgebra $M$ es una medida con signo que cumple $$\mu(E)=\inf\{\mu(V): E\subset V, V \text{ abierto}\}=\sup\{\mu(C): C\subset E, C\text{ cerrado}\}$$
para todo conjunto de Borel $E\in M$
\end{definicion}
En adelante, utilizaremos $I_n$ para referirnos al cubo unidad $n$-dimensional, $[0,1]^n$ y para el espacio de las medidas finitas con signo regulares de Borel sobre $I_n$ utilizaremos $M(I_n)$. Además, $C(I_n)$ denotará el espacio de funciones continuas en $I_n$.

\begin{definicion}\label{def:discriminatoria}
Una función $\sigma : \R \to \R$ es \emph{discriminatoria} si, para una medida $\mu \in M(I_n)$ con $$\int_{I_n} \sigma(w^Tx+b) d\mu(x)=0$$ para todo $ w\in R^n$ y $b\in\R$, implica que $\mu=0$.
\end{definicion}

\begin{lema}\label{thm:lema-discriminatoria-sigmoidal}
Cualquier función sigmoidal medible y acotada es discriminatoria. En particular, las funciones sigmoidales continuas son discriminatorias.
\end{lema}
\begin{proof}
Para cuales quiera $x,y\in\R^n$ y $\theta,\phi\R$, tendremos:

$$\sigma(\lambda(y^Tx+\theta)+\phi) \left \{ \begin{matrix}
\to 1 & \mbox{si } y^Tx+\theta>0 \mbox{ cuando } \lambda \to \infty, \\
\to 0 & \mbox{si } y^Tx+\theta<0 \mbox{ cuando } \lambda \to \infty, \\
= \sigma(\phi) & \mbox{si } y^Tx +\theta = 0 \; \forall \lambda
\end{matrix}\right.$$

Así, la sucesión de funciones $\sigma_\lambda(x)=\sigma(\lambda(y^Tx+\theta)+\phi)$ convergen puntualmente a la función

$$\gamma=\left\{ \begin{matrix}
=1 & \mbox{si } y^Tx+\theta>0 \\
=0 & \mbox{si } y^Tx+\thetaz0 \\
=\sigma(\phi) & \mbox{si } y^Tx+\theta>0
\end{matrix}\right.$$
cuando $\lambda\to+\infty$.\\

Sea $\Pi_{y,\theta}$ el hiperplano definido por $\{x | y^Tx+\theta =0\}$ y $H_{y,\theta}=\{x | y^Tx+\theta>0\}.$ Entonces por el teorema de convergencia dominada de lebesgue, tendremos que:

$$0\int_{I_n}\sigma_\lambda d\mu(x)=\int_{I_n}\gamma(x)d\mu(x)=\sigma(\phi)\mu(\Pi_{y,\theta})+\mu(H_{y,\theta})$$
para todo $\phi,\theta\in\R$ e $y\inR^n$.\\

A continuación, mostraremos que si $\mu(\Pi_{y,\theta})+\mu(H_{y,\theta})=0 \forall y \in \R^n, \; \forall \theta \in \R$ entonces $\mu=0$. Esto podría llegar a ser trivial si $\mu$ fuera una medida positiva pero en este caso no lo es.\\

Fijemos $y\in\R^n$. Para una función medible y acotada $h$, se define el funcional lineal $F$ como

$$F(h)=\int_{I_n}h(y^Tx)d\mu(x)$$
que será un funcional acotado sobre $L^\infty(\R)$ por ser $\mu$ una medida con signo finita. Sea $h$ la función indicadora en el intervalo $[0,+\infty)$ (es decir, $h(u)=1$ si $u\ge 0$ y $h(u)=0$ si $u<0$) por lo que

$$F(h)=\int_{I_n}h(y^Tx)d\mu(x)=\mu(\Pi_{y,-\theta})+\mu(H_{y,-\theta})=0.$$

De forma similar, $F(h)=0$ si $h$ es la función indicadora del intervalo abierto $(\theta,+\infty)$. Por la linealidad del operador, $F(h)=0$ para la función indicadora de cualquier intervalo y, por tanto, para cualquier función simple (esto es, sumas de funciones indicadoras de intervalos). Como las funciones simples son densas en $L^\infty(\R)$, $F=0$.\\

En particular, para las funciones medibles y acotadas $s(u)=sin(m\cdot u)$ y $c(u)=cos(m\cdot u)$ se tiene que

$$F(s+ic)=\int_{I_n} cos(m^Tx)+isin(m^Tx)d\mu(x)=\int_{I_n}exp(im^Tx)d\mu(x)=0$$

para todo $m$. Entonces, la transformada de Fourier $\mu$ es nula e igualmente debe de serlo $\mu$. Por tanto, $\sigma$ es discriminatoria.
\end{proof}

\begin{teorema}\label{thm:discriminatoria}
Sea $\sigma$ una función discriminatoria continua. Entonces, las sumas finitas de la forma $$G(x)=\sum_{i=1}^n \alpha_i \sigma(w_i^Tx+b)$$ son densas en $C(I_n)$. En otra palabras, dados $f\in C(I_n)$ y $\varepsilon>0$, existe una suma, $G(x)$, de la forma anterior, para la cual $$|G(x)-f(x)|<\varepsilon \;\;\;\;\; \forall x \in I_n.$$
\end{teorema}
\begin{proof}
Sea $S\subset C(I_n)$ un subespacio lineal de $C(I_n)$. Afirmaremos que la clausura de $S$ es el conjunto total $C(I_n)$.\\

Para ello, asumamos que la clasura de $S$, $\bar S$, no es todo el espacio $C(I_n)$. Entonces, $\bar S$ es un subespacio propio cerrado de $C(I_n)$ y, por el teorema de Hahn-Banach \cite{rudin1991functional}, existe un funcional lineal $L$ en $C(I_n)$ tal que $L \ne 0$ y $L(\bar S)=L(S)=0$.\\

Por el teorema de representación de Riesz \cite{rudin1987real}, este funcional lineal es de la forma

$$L(h)=\int_{I_n}h(x)d\mu(x)$$

para alguna medida $\mu\in M(I_n)$, para todo $h\in C(I_n)$. En particular, como $\sigma (y^T x +\theta)\in \bar S$ para todo $y\in\R^n$ y para todo $\theta \in \R$ tendremos que

$$\int_{I_n}\sigma(y^Tx+\theta)d\mu(x)=0$$

para todo $y\in I_n$ y para todo $\theta \in \R$. Como $\sigma$ era una función discriminatoria, se tendrá que $\mu=0$, que contradice que $F\ne 0$. Por tanto, concluimos que el subespacio $S$ es denso en $C(I_n)$.
\end{proof}
\begin{teorema}\label{thm:sigmoidal}
Sea $\sigma$ una función sigmoidal continua. Entonces, las sumas finitas de la forma $$G(x)=\sum_{i=1}^n \alpha_i \sigma(w_i^Tx+b)$$ son densas en $C(I_n)$. En otra palabras, dados $f\in C(I_n)$ y $\varepsilon>0$, existe una suma, $G(x)$, de la forma anterior, para la cual $$|G(x)-f(x)|<\varepsilon \;\;\;\;\; \forall x \in I_n.$$
\end{teorema}
\begin{proof}
Para esta demostración, se combina \autoref{thm:lema-discriminatoria-sigmoidal} y \autoref{thm:discriminatoria}, notando que las funciones sigmoidales continuas satisfacen las condiciones de este lema.
\end{proof}

\subsection{Kurt Hornik, 1989 y 1991}

En 1898 \cite{HornikEtAl89}, Hornik demostró que las redes neuronales con una capa oculta que utiliza funciones de aplastamiento (sigmoidal \autoref{def:sigmoidal} y no decreciente según la \emph{definición 2.3} del mismo artículo) arbitrarias son capaces de aproximar cualquier función Borel-medible de un espacio finito unidimensional con cualquier grado deseado de precisión.\\

Más tarde, en 1991 \cite{Kurt1991251}, extendió los teoremas de Cybenko mostrando que no necesariamente tenía que utilizar funciones de activación sigmoidales para los espacios de funciones considerados. A continuación enunciaremos el teorema para el espacio de funciones continuas.\\

\begin{teorema}
Sea $\sigma: \R \to \R$ una función continua, acotada y no constante. Entonces, las sumas finitas de la forma $$G(x)=\sum_{i=1}^n \alpha_i \sigma(w_i^Tx-b)$$ son densas en $C(X)$ para todos los subconjuntos compactos $X$ de $\R^m$.
\end{teorema}

\section{Profundidad indeterminada}

\subsection{Zhou Lu, Hongmin Pu, Feicheng Wang, Zhiquang Hu y Liwei Wang, 2017}
Estos autores demostraron \cite{2017arXiv170902540L} el caso de profundidad indeterminada para funciones Lebesgue-integrables y una función de activación ReLU. Este teorema fue presentado como una versión dual de las demostraciones para anchura indeterminada y abre el camino para nuevas demostraciones en el caso de anchura indeterminada.

\begin{teorema}
Para cualquier función Lebesgue-integrable $f:\R^n\to R$ y cualquier $\varepsilon>0$, existe $A$ una red totalmente conectada con función de activación ReLU y una anchura $d_m\leq n+4$, tal que la función $F_A$ representada por esta red satisface $$\int_{\R^n}|f(x)-F_A(x)|dx<\varepsilon.$$
\end{teorema}

\subsection{Patrick Kidger y Terry Lyons, 2019}
Una de las variantes más recientes \cite{2019arXiv190508539K}, fue presentada para el caso de una función de activación no afín, que sea continuamente diferenciable y con derivada no nula en al menos un punto. Destaca porque con sus consideraciones abarca las funciones de activación utilizadas en la práctica, incluyendo las funciones de activación polinómicas. En el artículo, se consideran otras extensiones o variaciones al teorema que enunciaremos.\\

\begin{definicion}
Sea $\sigma:\R\to\R$ y $n,m,k\in\N$. Entonces $NN_{n,m,k}^\sigma$ representa la clase de funciones $R^n\to \R^m$ descritas por una red neuronal hacia adelante con $n$ neuronas en la capa de entrada, $m$ neuronas en la capa de salida, y un numero arbitrario de capas ocultas, para las cuales $k$ neuronas tienen como función de activación la función $\sigma$. Cada neurona de la capa de salida tiene la función de activación identidad.
\end{definicion}

\begin{teorema}
Sea $\sigma : \R \to \R$ una función continua y no polinómica que es continuamente diferenciable en al menos un punto, con derivada no nula en dicho punto. Sea $K \subset \R^n$ un compacto. Entonces $NN_{n,m,n+m+2}^\sigma$ es denso en $C(K;\R^m)$ con respecto a la norma del supremo.
\end{teorema}

\begin{teorema}
Sea $\sigma : \R \to \R$ una función polinómica y no afín. Sea $K \subset \R^n$ un compacto. Entonces $NN_{n,m,n+m+2}^\sigma$ es denso en $C(K;\R^m)$ con respecto a la norma del supremo.
\end{teorema}

\chapter{Redes neuronales convolucionadas (CNN)} \label{ch:cnn}

Hasta ahora, todo lo que hemos hablado es válido para casi cualquier contexto puesto que no hemos hecho ninguna presunción en cuanto a la estructura de los datos o sobre las peculiaridades que estos presentan. Para nosotros todos los datos eran un vector con $D$ componentes a los que les correspondía una etiqueta y no le dábamos importancia a la estructura interna que estos pudieran llegar a presentar.\\

Para la edición y manipulación de imágenes, es común utilizar la operación de convolución como filtro. Las redes neuronales convolucionadas son aquellas que utilizan estos filtros, de ahí su nombre, para analizar las distintas características de las imágenes. Si bien, podemos fijar los pesos manualmente para utilizar algunos de los filtros más comunes en la edición de imágenes, se ha comprobado experimentalmente que por lo general se obtienen mejores resultados si es la propia red quien fija estos pesos a través de un proceso de aprendizaje.\\

%Anteriormente, comentábamos el ejemplo del conjunto de datos CIFAR-10 donde teníamos que $D=3072$ y mostrábamos cómo obteníamos ese valor. Para ello, utilizábamos la dimensión de la imagen de $ancho \times alto $ y la dimensión del \emph{espacio de color} que estábamos utilizando. De esta forma, si en lugar de trasladar estos valores a $\R^D$ como vectores nos quedamos en $M_{ancho,alto, \dim color}(\R)$ estaremos trabajando en un espacio de matrices tridimensionales. De esta forma, siendo $a$ el ancho, $h$ la altura y $c$ la dimensión del espacio de color, para cada matriz $m \in M_{a, h,c}(\R)$ tendremos que dado un píxel conocemos de forma inmediata sus píxeles más próximos sin necesidad de hacer ningún cálculo complejo. Nos aprovecharemos de esto para definir las \emph{capas convolucionadas} o \emph{convolutionals} y las \emph{capas de agrupación} o \emph{pooling}.\newline

En adelante, dejaremos de considerar que nuestros datos de entrada tienen una estructura de vector y consideraremos que estamos trabajando con matrices tridimensionales, es decir, nuestros datos estarán organizados como si fueran ortoedros. Esto lo representaremos como $M_{a,h,c}(\R)$ con $a,h,c\in \N$ donde $a$ representará la anchura, $h$ la altura y $c$ el número de canales. Para simplificarlo, cada entrada $X\in M_{a,h,c}(\R)$ representará $c$ imágenes en escala de grises de dimensiones $a\times h$. Como detalle, una imagen RGB está representada como tres imágenes en escala de grises($c=3$) donde cada canal representa un color primario y con la combinación de los tres obtenemos una imagen a color.

\section{Capas convolucionadas o \emph{convolutionals}}

Internamente, cada neurona de una capa convolucional posee un \emph{kernel} o \emph{filtro} $W\in M_{r,s,c}(\R)$ donde $r$, $s$ y $c$ son parámetros prefijados y una variable $b\in\R$ bias. Para cada entrada $X \in M_{a,h,c}(\R)$ tomamos una sección $x^{RS} \subset X$ donde $x^{RS}=(x_{ijk}^{RS})_{ijk}\; i=R,...,R+r, \;\; j=S,...,S+s\;$ y $\;k=1,...,c$ con $R=1,...,a-r$ y $S=1,...,h-s$. Así, cada neurona realiza una \emph{convolución matricial} y suma la variable $b$ bias: \newline

$$f(x^{RS})=\sum_{k=1}^{c}\sum_{i=1}^{r} \sum_{j=1}^{s} w_{i,j,k} \cdot x_{i+R,j+S,k}^{RS}+b$$

Siendo esta su función de puntuación de las neuronas correspondientes a dicho kernel. A este filtro le corresponderán tantas neuronas como sean necesarias para cubrir todos los datos de entrada. Una capa de convolución, podrá tener tantos filtros como se quieran y cada uno de ellos tendrá tantas neuronas como sean necesarias para cubrir toda la imagen. Visualmente, se transforma un ortoedro en otro.\newline
\begin{figure}
\centering

\begin{tikzpicture}
\coordinate (P1) at (0,0,0);
\coordinate (P2) at (0,0,1);
\coordinate (P3) at (0,1,0);
\coordinate (P4) at (0,1,1);
\coordinate (P5) at (1,0,0);
\coordinate (P6) at (1,0,1);
\coordinate (P7) at (1,1,0);
\coordinate (P8) at (1,1,1);


\draw[dashed,fill=gray!20] (P7) -- (P3) -- (P4) -- (P8);
\draw[dashed,fill=gray!20] (P1) -- (P2) -- (P4) -- (P3) -- (P1);
\draw[dashed,fill=gray!20] (P1) -- (P5) -- (P7) -- (P3) -- (P1);
\draw[dashed,fill=gray!20] (P2) -- (P6) -- (P5) -- (P0);
\draw (P2) -- (P4);
\draw (P2) -- (P6);
\draw (P3) -- (P4);
\draw (P3) -- (P7);
\draw (P4) -- (P8);
\draw (P5) -- (P7);
\draw (P5) -- (P6);
\draw (P6) -- (P8);
\draw (P7) -- (P8);

\begin{scope}[shift={(5,0,0)}]
\cubo
\draw[fill=gray!20] (P2) -- (0,0,0.7) -- (0,1,0.7) -- (P4) -- (P2);
\draw[fill=gray!20] (P2) -- (P4) -- (P8) -- (P6) -- (P2);
\draw[fill=gray!20] (P6) -- (1,0,0.7) -- (1,1,0.7) -- (P8) -- (P6);
\draw[fill=gray!20,dashed] (0,0,0.7) -- (1,0,0.7) -- (1,1,0.7) -- (0,1,0.7) -- (0,0,0.7);
\cubo
\end{scope}

\draw[->] (2,0,0) -- (4,0,0);

\end{tikzpicture}

\caption{Capa de convolución. La zona grisácea corresponde a la entrada de datos y al resultado de un filtro. }
\label{fig:convolution}
\end{figure}

Los píxeles pertenecientes a la sección de la capa anterior que utiliza cada neurona reciben el nombre de campo local receptivo y a la capa posterior generada por la aplicación de los filtros de la capa convolucionada recibe el nombre de capa de características.\\

\begin{figure}[htpb]
  \centering
  \includegraphics[width=0.45\textwidth]{cnn}
  \vrule
  \includegraphics[width=0.45\textwidth]{convolucion}
  \caption{Ejemplos de convoluciones \cite{stanford}(izq.) \cite{whatConvolucion}(der.). }
  \label{fig:convolucion}
\end{figure}
La cantidad de píxeles de desplazamiento que hay entre las distintas secciones se conoce paso, o \emph{stride}. Por otro lado, el relleno, o \emph{padding}, se trata de la opción de añadir, o no, valores temporales a la entrada original de datos con el fin de mantener la dimensión de los datos o evitar perder los valores asociados a las esquinas de las matrices de datos. En Keras \cite{chollet2015keras} se ofrecen las opciones de \emph{same} (\emph{zero-padding}), sin relleno y \emph{casual}.Tanto el paso como si la matriz es completada con algún otro número o no son parámetros que se prefijan al crear la capa, comunes a todos los filtros y controlan la dimensión de salida de la capa.\\

En esta \href{https://cs231n.github.io/assets/conv-demo/index.html}{demo} de \cite{stanford} podemos ver el funcionamiento de dos filtros $3\times 3$ ($r=3,s=3$) a una entrada $x\in M_{7,7,3}$. Nótese, que el filtro no es aplicado en todas las submatrices sino que avanza dos posiciones tanto vertical como horizontalmente, es decir, la capa posee un paso, o \emph{stride}, de $2$. En la formulación anterior, se ha supuesto que el paso es de tamaño $1$. Además, en la demo se ha completado la matriz $x$ con $0$ hasta tener una dimensión de $9\times 9\times 3$ para asegurarnos de que recorremos todas las posiciones de $x$ con el filtro. \\

Se debe hacer mención de la operación de deconvolución o convolución traspuesta que funciona de forma similar a la convolución clásica. Esta operación pretende ir de forma inversa a una convolución, tratando de recuperar los valores originales antes de una convolución. Uno de sus usos más importantes es el muestreo ascendente, o \emph{up sampling}.

\section{Capas de Agrupación o Pooling}
Debido a la gran cantidad de información generada a través de las convoluciones en los mapas de características, es necesario condensar esta información para obtener sólo aquella que sea realmente relevante siendo conocido este procedimiento como submuestreo, o \emph{downsampling}. Para ello se suele utilizar las capas de agrupación, o \emph{pooling layers}, que resume la información utilizando diversos algoritmos. Como ejemplos de algoritmos de \emph{pooling} tenemos la agrupación a través del valor máximo y la agrupación a través del valor medio que calculan, respectivamente, el valor máximo y el valor medio del campo local receptivo. Para estas capas también existen los conceptos de filtros y de paso.\\
\begin{figure}[htpb]
  \centering
  \includegraphics[width=0.35\textwidth]{pool}
  \vrule
  \includegraphics[width=0.55\textwidth]{maxpool}
  \caption{Ejemplos de agrupaciones, o \emph{pooling} \cite{stanford}. }
  \label{fig:pooling}
\end{figure}

%\section{Ejemplos}

%FIXME: ¿Apéndices o distribuido en el pdf? Cosas a mencionar:

%\begin{itemize}
%\item Cómo se ven tras cada neurona y/o capa (cómo pooling y convolution modifican la imagen)
%\item Cómo los filtros modifican una imagen visualmente
%\end{itemize}

\chapter{Operador no local}\label{def:non-local}
En la sección anterior se ha visto que una operación de convolución posee un kernel que limita la cantidad de posiciones que son observadas, de forma que la cantidad de parámetros de la capa de convolución es dependiente al tamaño de este núcleo. Esto tiene como desventaja que, para cada posición de nuestra entrada de datos, las posiciones que influyen al resultado de la convolución están restringidas a un subconjunto de tamaño fijo de los datos de entrada por lo que se suele decir que es una operación \emph{local}. Si quisiéramos utilizar todos los datos de entrada para cada posición de la imagen, tendríamos un aumento considerable de parámetros a utilizar, con los consecuentes problemas de memoria y entrenamiento.\\

El objetivo de esta sección es definir un tipo de operación que sea capaz de extraer características de nuestros datos de entrada utilizando toda su dimensión y sin tener un aumento drástico del número de parámetros a entrenar, será a lo que llamaremos un operador no local. La necesidad de este tipo de operación nace del hecho de que, por lo general, las imágenes a estudiar no suelen ser un conjunto de \emph{collage} sin relación entre sí, sino que el entorno de nuestra imagen puede llegar a ser de utilidad a la hora de distinguir qué objetos estamos clasificando.\\

Para simplificar la definición, nos restringiremos al contexto de espacios vectoriales de dimensión finita sobre el cuerpo de los números reales.
\begin{definicion}
 Sea $x=(x_1,...,x_D)\in \R^D$ una señal de entrada, $u:\R \times \R \to \R$ un producto escalar, $v:\R \to \R$ función real evaluada y $C:\R^D \to \R$.  Se define una operación no local genérica o \emph{generic non-local operation} $f:\R^D \to \R^D$ con $f(x)=(f_1(x_1),...,f_D(x_D))$ aquella donde \cite{DBLP:journals/corr/abs-1711-07971}:

 $$f_i(x_i)=\frac{1}{C(x)}\sum_{j=1}^{D} u(x_i,x_j)v(x_j) \; \;  \forall i=1,...,D$$
\end{definicion}

 En este contexto, $v(x_j)$ será una representación de la señal de $x$ entrada en la posición $j$, $u$ representará una relación, por ejemplo la afinidad, entre las posiciones $i$ y $j$. %La operación recibe este nombre debido a que considera todas las posiciones de la imagen para calcular el valor del filtro en la posición $i$ distinguiéndose así de una operación convolucional que se centraría en un rango \emph{local} de posiciones.% ($i-1\leq j \leq i+1$ en el caso de tener una convolución de núcleo de tamaño $3$).
\newline

 Proseguiremos enunciando varios ejemplos de funciones que pertenezcan a dicha definición para mostrar la diversidad que podemos elegir a la hora de implementar un modelo que siga esta estructura. Por simplicidad, consideraremos que $v(x_j)=W_v x_j$ donde $W_v$ es una matriz de pesos que tiene que ser aprendida, siendo esta fácilmente implementable como una convolución de núcleo de tamaño $1$ \cite{Buades:2005:NAI:1068508.1069066} \cite{DBLP:journals/corr/abs-1711-07971}.

 \begin{enumerate}
 \item \emph{Gaussiana} $u(x_i,x_j)=e^{x_i^T x_j}$ y $C(x)= \sum_{j=1}^D u(x_i,x_j)$.
 \item \emph{Embedded Gaussian} $u(x_i,x_j)=e^{\phi_i(x_i)^T,\theta_j(x_j)}$ donde $\phi_i(x_i)=W_{\phi_i} x_i$ y $\theta_j(x_j)=W_{\theta_j} x_j$ y  $C(x)= \sum_{j=1}^D u(x_i,x_j)$. Nótese que el módulo \emph{self-attention} \cite{DBLP:journals/corr/VaswaniSPUJGKP17} es un caso particular de este ejemplo.
 \item \emph{Dot-product similarity} $u(x_i,x_j)=\phi_i(x_i)^T \theta_j(x_j)$ donde $\phi_i(x_i)=W_{\phi_i} x_i$, $\theta_j(x_j)=W_{\theta_j} x_j$ y $C(x)=D$.
 \item \emph{Non-local means} \cite{Buades:2005:NAI:1068508.1069066}
 % ¿Concatenacion?
 \end{enumerate}

Para poder afirmar que un operador no local cumple con lo que buscamos, primero se debe demostrar que las operaciones de este tipo nos dan el resultado deseado. Para ello, primero deberemos de enunciar una serie de conceptos.

\section{Conceptos previos}
En adelante, el conjunto $\Omega$ será un espacio muestral y $\mathscr{A}$ será una \emph{$\sigma$-álgebra} sobre dicho $\Omega$. Además, $P$ será una probabilidad aplicada a los elementos de la $\sigma$-álgebra $\mathscr{A}$. Esto será representado como $(\Omega,\mathscr{A},P)$ siendo un espacio probabilístico y será donde trabajemos.

\begin{definicion}[Proceso estocástico]\label{def:pe}
  Un \emph{proceso estocástico} es una familia de variables aleatorias ${\{X_t\}}_{t \in T}$ en $T$, donde $T$ es un conjunto ordenado arbitrario y cada variable aleatoria está definida sobre un espacio de probabilidad $(\Omega, \mathscr{A}, P)$.
\end{definicion}

A partir de ahora, siempre que nos refiramos a un proceso nos estaremos refiriendo a un proceso estocástico bajo la definición anterior. \\

\begin{definicion}[Proceso con incrementos independientes]
  Un proceso estocástico tiene \emph{incrementos independientes} si $\forall n > 1, \forall t_1 < \ldots < t_n \in T$ $$X_{t_1}, X_{t_2} - X_{t_1}, \ldots, X_{t_n} - X_{t_{n-1}} \text{son variables aleatorias independientes.}$$
\end{definicion}

\begin{definicion}[Proceso con incrementos estacionarios]
  Un proceso estocástico tiene \emph{incrementos estacionarios} si $\forall s < t \in T, \forall h\in T$  $$(X_t - X_s) \sim (X_{t+h} - X_{s+h}).$$
\end{definicion}

\begin{nota}
  $X \sim Y$ indica que $X$ sigue la misma distribución que $Y$.
\end{nota}

\begin{definicion}[Proceso estacionario]
  Un proceso estocástico es \emph{estrictamente estacionario} si $\forall n, \forall t_1 < \ldots < t_n \in T, \forall h \in T$ $$(X_{t_1}, \ldots, X_{t_n}) \sim (X_{t_1 + h}, \ldots, X_{t_n + h}).$$
\end{definicion}

A continuación, definiremos el concepto proceso mezclado. En la literatura, los cuatro tipos de mezcla normalmente son referidos como $\psi$-mezclado, $\Phi$-mezclado (o uniformemente mezclado), $\rho$-mezclado (o mezcla basada en la correlación maximal) y $\alpha$-mezclado (o fuertemente mezclado). Cambiaremos la notación a $\Phi_i$-mezcla $i=1,...,4$ respectivamente, de forma que una $\Phi_i$-mezcla implique una $\Phi_{i+1}$-mezcla con $i=1,...,3$. \cite{RePEc:eee:spapps:v:36:y:1990:i:1:p:107-116}\\

Por notación, diremos también que $\mathscr{A}_m^n$ será la $\sigma$-álgebra inducida por las variables aleatorias $Z_j$ con $m\le j \le n$ en el espacio muestral $\Omega$. Entonces:\\

\begin{definicion}[Proceso mezclado]\label{def:mezcla}
La secuencia $\{Z_j\}_{j\ge 1}$ es conocida $\Phi_i$-mezclada con $i=1,...,4$ si, para cada $A \in \mathscr{A}_1^k$ y para cada $B\in \mathscr{A}_{k+n}^{\infty}$, las siguientes desigualdades son satisfechas \cite{RePEc:eee:spapps:v:36:y:1990:i:1:p:107-116}:
\begin{itemize}
\item Para $\Phi_1$-mezclada: $$|P(A\cap B) - P(A)P(B)| \le \Phi_1 (n)P(A)P(B)\text{ con } \Phi_1(n)\downarrow 0 \text{ cuando } n\to \infty$$
\item Para $\Phi_2$-mezclada: $$|P(A\cap B) - P(A)P(B)| \le \Phi_2 (n)P(A)\text{ con } \Phi_2(n)\downarrow 0 \text{ cuando } n\to \infty$$
\item Para $\Phi_3$-mezclada: $$|P(A\cap B) - P(A)P(B)| \le \Phi_3 (n)[P(A)P(B)]^\frac{1}{2} \text{ con } \Phi_3(n)\downarrow 0 \text{ cuando } n\to \infty$$
\item Para $\Phi_4$-mezclada: $$|P(A\cap B) - P(A)P(B)| \le \Phi_4 (n) \text{ con } \Phi_4(n)\downarrow 0 \text{ cuando } n\to \infty$$
\end{itemize}
\end{definicion}

\section{Consistencia de un operador no local}
La idea es que, bajo supuestos estacionarios, para cada píxel $i$ una operación no local converge a la expectativa condicional de un píxel $i$ observado en un vecindario del píxel. En este caso las condiciones de estacionariedad equivalen a decir que, a medida que crece el tamaño de la imagen, podemos encontrar muchas regiones similares para todos los detalles de la imagen. Es decir, conforme aumenta el tamaño de la imagen es más probable que, si encontramos un objeto perteneciente a determinada categoría, encontremos más objetos pertenecientes a dicha categoría.\\

Sea $V$ un campo aleatorio (\emph{random field}) y los datos de entrada $v$ una realización de $V$. Sea $Z$ una secuencia de variables aleatorias con $Z_i=\{X_i,Y_i\}$ con $Y_i=V(i)$ es real valuada y $X_i=V(I\backslash \{i\}) \R^p$ valuada donde $I$ representa el conjunto de posiciones. Una operación no local genérica será un estimador de la esperanza condicionada $E[Y_i|X_i=v(I\backslash{i})]$. \cite{Buades:2005:NAI:1068508.1069066}

\begin{teorema}\cite{Buades:2005:NAI:1068508.1069066}
Sea $Z=\{V(N_i\backslash{i}),V(i)\}_{i\ge 1}$ un proceso estocástico estrictamente estacionario y mezclado. Sea $f^n$ una operación no local genérica aplicada a la secuencia $Z_n=\{V(N_i\backslash\{i\}),V(i)\}_{i\ge 1}^n$. Entonces, $$ |f^n(j)-[Y_j|X_j=v(I\backslash \{j\})]| \to 0 \; a.s \;  \forall j \in{1,...,n}$$.
\end{teorema}

En un contexto más general, la demostración se puede encontrar en \cite{RePEc:eee:spapps:v:36:y:1990:i:1:p:107-116} bajo la suposición de cualquiera de cuatro tipos de procesos mezclados enunciados en \autoref{def:mezcla}
